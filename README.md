# harm_detection_via_sparse_latents

problem motivation: Harmful Adversial Prompt Detection


![Diagram]([https://github.com/username/my-project/blob/main/assets/diagram.png](https://github.com/xinrose-lin/locating_latent_rep_of_harm/blob/main/report/Capstone-Research-Poster-Lin-Xin-Rose.jpg)

Research Question: can we detect harmful prompts via internal representation of language models?

In simple terms, 

related fields

This project investigate

