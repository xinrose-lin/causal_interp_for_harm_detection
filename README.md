# harm_detection_via_sparse_latents

Problem Motivation: Harmful Adversial Prompt Detection - Given a prompt, can the model detect if it is harmful/adversarial?

![Diagram](https://github.com/xinrose-lin/locating_latent_rep_of_harm/blob/main/report/Capstone-Research-Poster-Lin-Xin-Rose.jpg)

Research Question: can we detect harmful prompts via internal representation of language models?

In simple terms, 

related fields

This project investigate

