{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get model 512 activations \n",
    "- for harmful vs nonharmful data\n",
    "\n",
    "- qual plot: data activation distribution does not seem very different at all.. \n",
    "\n",
    "- can model detect harmful prompts based on probe trained on 512 activations alone \n",
    "\n",
    "- can it do so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from sae_lens import HookedSAETransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.dataset import read_from_pt_gz, save_to_pt_gz, load_target_concept_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pythia with hooked transformer for consistent comparison\n",
    "\n",
    "device = \"cpu\"\n",
    "pythia_model: HookedSAETransformer = HookedSAETransformer.from_pretrained('EleutherAI/pythia-70m-deduped', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (data, labels)\n",
    "nonharmful_data = load_target_concept_data(train=True, target_label=0)\n",
    "harmful_data = load_target_concept_data(train=True, target_label=1)\n",
    "\n",
    "logits_no_saes, cache_no_saes = pythia_model.run_with_cache(nonharmful_data[0])\n",
    "save_to_pt_gz(\"sparse_acts/train/nonharmful_acts_512.pt.gz\", cache_no_saes['blocks.4.hook_resid_post'])\n",
    "nonharmful_acts = read_from_pt_gz(\"sparse_acts/train/nonharmful_acts_512.pt.gz\")\n",
    "\n",
    "logits_no_saes, cache_no_saes = pythia_model.run_with_cache(harmful_data[0])\n",
    "save_to_pt_gz(\"sparse_acts/train/harmful_acts_512.pt.gz\", cache_no_saes['blocks.4.hook_resid_post'])\n",
    "harmful_acts = read_from_pt_gz(\"sparse_acts/train/harmful_acts_512.pt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonharmful_acts = read_from_pt_gz(\"sparse_acts/train/nonharmful_acts_512.pt.gz\")\n",
    "harmful_acts = read_from_pt_gz(\"sparse_acts/train/harmful_acts_512.pt.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nonharmful_data = nonharmful_acts.mean((0, 1))  # mean along axes 0 and 1 for nonharmful_acts\n",
    "harmful_data = harmful_acts.mean((0, 1))        # mean along axes 0 and 1 for harmful_acts\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot nonharmful_acts boxplot at position 1 with a custom color\n",
    "box_nonharmful = plt.boxplot(nonharmful_data, positions=[1], patch_artist=True, boxprops=dict(facecolor='lightblue', color='blue'))\n",
    "\n",
    "# Plot harmful_acts boxplot at position 2 with a custom color\n",
    "box_harmful = plt.boxplot(harmful_data, positions=[2], patch_artist=True, boxprops=dict(facecolor='lightcoral', color='red'))\n",
    "\n",
    "# Function to annotate max, min, and median values\n",
    "def annotate_boxplot(data, pos):\n",
    "    # Extract the statistics from the boxplot\n",
    "    median = t.median(data)\n",
    "    minimum = t.min(data)\n",
    "    maximum = t.max(data)\n",
    "\n",
    "    plt.text(pos + 0.1, minimum, f\"Min: {minimum:.2f}\", ha='left', va='center', fontsize=10, color='blue')\n",
    "    plt.text(pos + 0.1, median, f\"Median: {median:.2f}\", ha='left', va='center', fontsize=10, color='green')\n",
    "    plt.text(pos + 0.1, maximum, f\"Max: {maximum:.2f}\", ha='left', va='center', fontsize=10, color='red')\n",
    "\n",
    "# Annotate both boxplots\n",
    "annotate_boxplot(nonharmful_data, 1)\n",
    "annotate_boxplot(harmful_data, 2)\n",
    "\n",
    "# Set the x-axis labels\n",
    "plt.xticks([1, 2], ['Non-Harmful Acts', 'Harmful Acts'])\n",
    "\n",
    "# Optional: Add labels and title\n",
    "plt.xlabel('Type of Acts')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.title('Comparison of Non-Harmful vs Harmful Activations (dim=512)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "## quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean for final token\n",
    "\n",
    "nonharmful_data = nonharmful_acts[:, -1, :].mean(0)  # mean along axes 0 and 1 for nonharmful_acts\n",
    "harmful_data = harmful_acts[:, -1, :].mean(0)      # mean along axes 0 and 1 for harmful_acts\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot nonharmful_acts boxplot at position 1 with a custom color\n",
    "box_nonharmful = plt.boxplot(nonharmful_data, positions=[1], patch_artist=True, boxprops=dict(facecolor='lightblue', color='blue'))\n",
    "\n",
    "# Plot harmful_acts boxplot at position 2 with a custom color\n",
    "box_harmful = plt.boxplot(harmful_data, positions=[2], patch_artist=True, boxprops=dict(facecolor='lightcoral', color='red'))\n",
    "\n",
    "# Function to annotate max, min, and median values\n",
    "def annotate_boxplot(data, pos):\n",
    "    # Extract the statistics from the boxplot\n",
    "    median = t.median(data)\n",
    "    minimum = t.min(data)\n",
    "    maximum = t.max(data)\n",
    "\n",
    "    plt.text(pos + 0.1, minimum, f\"Min: {minimum:.2f}\", ha='left', va='center', fontsize=10, color='blue')\n",
    "    plt.text(pos + 0.1, median, f\"Median: {median:.2f}\", ha='left', va='center', fontsize=10, color='green')\n",
    "    plt.text(pos + 0.1, maximum, f\"Max: {maximum:.2f}\", ha='left', va='center', fontsize=10, color='red')\n",
    "\n",
    "# Annotate both boxplots\n",
    "annotate_boxplot(nonharmful_data, 1)\n",
    "annotate_boxplot(harmful_data, 2)\n",
    "\n",
    "# Set the x-axis labels\n",
    "plt.xticks([1, 2], ['Non-Harmful Acts', 'Harmful Acts'])\n",
    "\n",
    "# Optional: Add labels and title\n",
    "plt.xlabel('Type of Acts')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.title('Comparison of Non-Harmful vs Harmful Activations (dim=512)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "## quite similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quantitative results: train with activations of last token \n",
    "\n",
    "<!-- since there is no signficant differences  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returns batches of activations\n",
    "def data_loader(data, labels, batch_size=16, seed = 42, device=\"cpu\"):\n",
    "    idxs = list(range(len(data)))\n",
    "    # creates a shuffled list \n",
    "    random.Random(seed).shuffle(idxs)\n",
    "    # get data in this shuffled order\n",
    "    data, labels = [data[i] for i in idxs], [labels[i] for i in idxs]\n",
    "    # return the batches\n",
    "    batches = [\n",
    "        (data[i:i+batch_size], t.tensor(labels[i:i+batch_size], device=device)) for i in range(0, len(data), batch_size)\n",
    "    ]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sice \n",
    "last_tok_acts_data = t.cat((nonharmful_acts[:, -1, :], harmful_acts[:, -1, :]), dim=0).tolist()\n",
    "label = nonharmful_data[1] + harmful_data[1]\n",
    "\n",
    "batches = data_loader(last_tok_acts_data, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_probe(batches, lr=1e-2, epochs=1, dim=512, seed=42, probe=\"linear\"):\n",
    "    t.manual_seed(seed)\n",
    "    if probe == \"linear\":\n",
    "        probe = Probe(dim)\n",
    "    else: \n",
    "        print('define probe')\n",
    "\n",
    "    optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    losses = []\n",
    "    epoch_losses = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # epoch_loss = 0\n",
    "        for batch in batches:\n",
    "            \n",
    "            acts = batch[0]\n",
    "            labels = batch[1] \n",
    "            logits = probe(acts)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    return probe, losses, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe = Probe(512)\n",
    "optimizer = t.optim.AdamW(probe.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train_probe_only(probe, batches, lr=1e-2, epochs=1, dim=512, seed=42, probe=\"linear\"):\n",
    "    \n",
    "    losses = []\n",
    "    # epoch_losses = []\n",
    "    \n",
    "    for batch in batches:\n",
    "        \n",
    "        acts = batch[0]\n",
    "        labels = batch[1] \n",
    "        logits = probe(acts)\n",
    "        loss = criterion(logits, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return probe, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model \n",
    "\n",
    "## compute time for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe, losses, epoch_loss = train_probe(batches, epochs=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_loss)\n",
    "plt.title(\"(dim=512) original concept probe training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_probe(probe, batches, seed=42):\n",
    "    with t.no_grad():\n",
    "        corrects = []\n",
    "\n",
    "        for batch in batches:\n",
    "            acts = batch[0]\n",
    "            labels = batch[1]\n",
    "            logits = probe(acts)\n",
    "            preds = (logits > 0.0).long()\n",
    "            # print(logits)\n",
    "            # print(preds)\n",
    "            corrects.append((preds == labels).float())\n",
    "        return t.cat(corrects).mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/test_data.json\", \"r\") as file: \n",
    "#     test_data = json.load(file)\n",
    "# len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (data, labels)\n",
    "nonharmful_data = load_target_concept_data(train=False, target_label=0)\n",
    "harmful_data = load_target_concept_data(train=False, target_label=1)\n",
    "\n",
    "logits_no_saes, cache_no_saes = pythia_model.run_with_cache(nonharmful_data[0])\n",
    "save_to_pt_gz(\"sparse_acts/test/nonharmful_acts_512.pt.gz\", cache_no_saes['blocks.4.hook_resid_post'])\n",
    "test_nonharmful_acts = read_from_pt_gz(\"sparse_acts/test/nonharmful_acts_512.pt.gz\")\n",
    "\n",
    "logits_no_saes, cache_no_saes = pythia_model.run_with_cache(harmful_data[0])\n",
    "save_to_pt_gz(\"sparse_acts/test/harmful_acts_512.pt.gz\", cache_no_saes['blocks.4.hook_resid_post'])\n",
    "test_harmful_acts = read_from_pt_gz(\"sparse_acts/test/harmful_acts_512.pt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sice \n",
    "last_tok_acts_data = t.cat((test_nonharmful_acts[:, -1, :], test_harmful_acts[:, -1, :]), dim=0).tolist()\n",
    "label = nonharmful_data[1] + harmful_data[1]\n",
    "\n",
    "test_batches = data_loader(last_tok_acts_data, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce batches observe performance\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_test_acc = []\n",
    "epoches = 25\n",
    "\n",
    "\n",
    "probe, losses, epoch_loss = train_probe(batches=train_batches) \n",
    "test_accuracy = test_probe(probe, batches=test_batches, seed=42)\n",
    "# epoch_train_loss.append(losses[-1])\n",
    "epoch_test_acc.append(test_accuracy)\n",
    "\n",
    "### hmm should test accuracy inside the training loop? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## measure score on harmful data only \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## measure classification score on advbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## measure score on autodan\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
